{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "\n",
    "\\*\\* Difference between SGD and back propogation:- basically back\n",
    "propogation is just the method to calculate gradient of the algorithmic\n",
    "function formed for our neural network,whereas SGD uses gradient of the\n",
    "function in order to learn for the neural network and to improve\n",
    "accuracy,predictions and mistakes.\n",
    "\n",
    "\\*\\* why to use SGD in a neural network? \\*\\* SGD is used mainly because\n",
    "of 2 reasons :-\n",
    "\n",
    "-   fast :- as compared to other methods its realtively faster and\n",
    "    convenient to work.\n",
    "\n",
    "-   cost :- SGD manages the cost of the neural network efficiently.\n",
    "\n",
    "\\*\\* working \\*\\* it generally takes the few training set values . using\n",
    "very less training set it compute the cost and gradient of the training\n",
    "set. it contains a (alpha) which is called a learning rate. in SGD the\n",
    "learning rate is small as compared to other methods. we need to set\n",
    "learning rate and schedule in a way to maximise the accuracy and\n",
    "minimize the error. so we set small alpha that provides a stable\n",
    "convergence and we reduce the alpha as teh convergence rate slows down.\n",
    "\n",
    "one important thing is that data must be provided in a well shuffled\n",
    "manner to avoid bias of the gradient."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
